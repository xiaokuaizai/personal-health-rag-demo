{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c2fb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 核心依赖库\n",
    "import oss2\n",
    "import dashscope\n",
    "import json\n",
    "import numpy as np\n",
    "import logging\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "# 配置日志（便于查看运行信息）\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1967d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_health_file(file_path):\n",
    "    \"\"\"\n",
    "    读取健康文件（支持.txt和.json格式），提取纯文本内容\n",
    "    :param file_path: 本地文件路径（如\"test_blood_pressure.txt\"）\n",
    "    :return: 提取的纯文本内容\n",
    "    \"\"\"\n",
    "    # 先判断文件后缀，区分格式\n",
    "    if file_path.endswith(\".txt\"):\n",
    "        # 读取txt文件：直接读取全部文本\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text_content = f.read().strip()  # 去除首尾空白字符\n",
    "            return text_content\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"读取txt文件失败：{str(e)}\")\n",
    "    elif file_path.endswith(\".json\"):\n",
    "        # 读取json文件：提取content字段的文本（和测试json模板对应）\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                json_data = json.load(f)\n",
    "            # 从json中获取content字段，若不存在返回空字符串\n",
    "            text_content = json_data.get(\"content\", \"\").strip()\n",
    "            return text_content\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"读取json文件失败：{str(e)}\")\n",
    "    else:\n",
    "        # 不支持的文件格式\n",
    "        raise ValueError(\"仅支持.txt和.json格式的文件\")\n",
    "\n",
    "# 测试函数（可选运行，验证文件读取是否正常）\n",
    "# test_text = read_health_file(\"test_blood_pressure.txt\")\n",
    "# print(f\"读取的文本内容：\\n{test_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a711046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== 替换为你的DashScope API Key ======================\n",
    "DASHSCOPE_API_KEY = \"sk-7465f14e980d4eb9b1578901db5c5311\"  # 改成你自己申请的API Key\n",
    "# =========================================================================\n",
    "dashscope.api_key = DASHSCOPE_API_KEY\n",
    "\n",
    "def text_to_vector(text):\n",
    "    \"\"\"\n",
    "    将文本转为768维向量（使用阿里云DashScope text-embedding-v1模型）\n",
    "    :param text: 待向量化的纯文本\n",
    "    :return: 一维向量列表（长度768）\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        raise ValueError(\"待向量化的文本不能为空\")\n",
    "    \n",
    "    try:\n",
    "        # 调用DashScope Embedding API\n",
    "        response = dashscope.TextEmbedding.call(\n",
    "            model=dashscope.TextEmbedding.Models.text_embedding_v1,\n",
    "            input=text\n",
    "        )\n",
    "        # 判断请求是否成功\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"向量化API调用失败：{response.message}\")\n",
    "        # 提取向量数据\n",
    "        embedding_vector = response.output[\"embeddings\"][0][\"embedding\"]\n",
    "        return embedding_vector\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"文本向量化失败：{str(e)}\")\n",
    "\n",
    "# 测试函数（可选运行，验证向量化是否正常）\n",
    "# test_text = \"2023-10-27 血压：145/95mmHg，略高于正常范围\"\n",
    "# test_vector = text_to_vector(test_text)\n",
    "# print(f\"向量维度：{len(test_vector)}\")\n",
    "# print(f\"向量前10位：{test_vector[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76d7ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== 替换为你的阿里云普通Bucket配置 ======================\n",
    "ALI_ACCESS_KEY_ID = \"LTAI5t7V2vGe6mMP8p7NM9H6\"        # 改成你的AccessKey ID\n",
    "ALI_ACCESS_KEY_SECRET = \"v7uQu0krg47pBvE28uhfh6ExyklTLB\"  # 改成你的AccessKey Secret\n",
    "BUCKET_NAME = \"xkz1\"                            # 你的Bucket名称（无需修改，和页面一致）\n",
    "ENDPOINT = \"oss-cn-beijing.aliyuncs.com\"        # 关键修正：对应你页面的外网Endpoint，已加oss-前缀\n",
    "# ===========================================================================\n",
    "\n",
    "# 核心依赖库（若单元格1已导入，此处可忽略；若单独运行，需保留）\n",
    "import oss2\n",
    "import json\n",
    "import logging\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "# 配置日志（便于查看运行信息）\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 初始化普通Bucket客户端（使用正确的Endpoint，已修正地址拼接问题）\n",
    "auth = oss2.Auth(ALI_ACCESS_KEY_ID, ALI_ACCESS_KEY_SECRET)\n",
    "bucket = oss2.Bucket(auth, ENDPOINT, BUCKET_NAME)\n",
    "\n",
    "def upload_to_common_bucket(obj_key: str, vector: List[float], original_text: str, metadata: Dict[str, Any]) -> bool:\n",
    "    \"\"\"\n",
    "    向普通OSS Bucket上传（向量+原始文本+元数据）\n",
    "    :param obj_key: 自定义对象名称（如\"health_record_bp_001.json\"）\n",
    "    :param vector: 文本向量\n",
    "    :param original_text: 原始文本\n",
    "    :param metadata: 元数据字典\n",
    "    :return: 上传成功返回True，失败返回False\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 组装完整数据\n",
    "        full_data = {\n",
    "            \"vector\": vector,\n",
    "            \"original_text\": original_text,\n",
    "            \"metadata\": metadata\n",
    "        }\n",
    "        # 序列化为JSON字符串\n",
    "        json_data = json.dumps(full_data, ensure_ascii=False)\n",
    "        # 上传到普通Bucket\n",
    "        bucket.put_object(obj_key, json_data.encode(\"utf-8\"))\n",
    "        logger.info(f\"成功上传对象：{obj_key}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"上传失败：{str(e)}\")\n",
    "        return False\n",
    "\n",
    "def pull_all_from_common_bucket(prefix: Optional[str] = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    从普通Bucket拉取指定前缀的所有数据\n",
    "    :param prefix: 对象前缀（如\"health_record_\"）\n",
    "    :return: 反序列化后的数据集\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    try:\n",
    "        # 遍历Bucket中的对象\n",
    "        for obj in oss2.ObjectIterator(bucket, prefix=prefix):\n",
    "            # 下载并解码内容\n",
    "            obj_content = bucket.get_object(obj.key).read().decode(\"utf-8\")\n",
    "            # 反序列化\n",
    "            full_data = json.loads(obj_content)\n",
    "            all_data.append(full_data)\n",
    "        logger.info(f\"拉取到{len(all_data)}条数据\")\n",
    "        return all_data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"拉取失败：{str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59e0592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 读取健康文件（复用上面的函数）\n",
    "def load_health_data(file_path: str) -> str:\n",
    "    \"\"\"读取健康文件文本\"\"\"\n",
    "    try:\n",
    "        text = read_health_file(file_path)\n",
    "        logger.info(f\"成功读取文件：{file_path}，文本长度：{len(text)}\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        logger.error(f\"读取文件失败：{str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "# 2. 向量化并上传到普通Bucket\n",
    "def process_and_upload(file_path: str, obj_key: str, metadata: Dict[str, Any]) -> bool:\n",
    "    \"\"\"\n",
    "    完整流程：读取文件→向量化→上传普通Bucket\n",
    "    \"\"\"\n",
    "    # 1. 读取文件\n",
    "    original_text = load_health_data(file_path)\n",
    "    if not original_text:\n",
    "        return False\n",
    "    # 2. 文本向量化\n",
    "    try:\n",
    "        vector = text_to_vector(original_text)\n",
    "        logger.info(f\"成功生成向量，向量维度：{len(vector)}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"向量化失败：{str(e)}\")\n",
    "        return False\n",
    "    # 3. 上传普通Bucket\n",
    "    return upload_to_common_bucket(obj_key, vector, original_text, metadata)\n",
    "\n",
    "# 3. 从普通Bucket拉取并语义检索\n",
    "def search_from_common_bucket(\n",
    "    query_text: str,\n",
    "    filter_metadata: Optional[Dict[str, Any]] = None,\n",
    "    return_top_k: int = 1,\n",
    "    obj_prefix: Optional[str] = \"health_record_\"\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    从普通Bucket检索：拉取数据→过滤元数据→计算相似度→返回结果\n",
    "    \"\"\"\n",
    "    # 1. 校验入参\n",
    "    if not query_text:\n",
    "        logger.error(\"查询文本不能为空\")\n",
    "        return []\n",
    "    if return_top_k < 1:\n",
    "        return_top_k = 1\n",
    "    # 2. 从普通Bucket拉取所有数据\n",
    "    all_data = pull_all_from_common_bucket(prefix=obj_prefix)\n",
    "    if not all_data:\n",
    "        logger.info(\"普通Bucket中无匹配数据\")\n",
    "        return []\n",
    "    # 3. 查询语句向量化\n",
    "    try:\n",
    "        query_vector = np.array(text_to_vector(query_text))\n",
    "        query_norm = np.linalg.norm(query_vector)\n",
    "        if query_norm == 0:\n",
    "            raise ValueError(\"查询向量模长为0，无法计算相似度\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"查询语句向量化失败：{str(e)}\")\n",
    "        return []\n",
    "    # 4. 元数据过滤\n",
    "    filtered_data = all_data\n",
    "    if filter_metadata and isinstance(filter_metadata, dict):\n",
    "        filtered_data = [\n",
    "            item for item in all_data\n",
    "            if all(item[\"metadata\"].get(k) == v for k, v in filter_metadata.items())\n",
    "        ]\n",
    "    if not filtered_data:\n",
    "        logger.info(\"无符合元数据过滤条件的数据\")\n",
    "        return []\n",
    "    # 5. 批量计算余弦相似度\n",
    "    item_vectors = np.array([item[\"vector\"] for item in filtered_data])\n",
    "    item_norms = np.linalg.norm(item_vectors, axis=1)\n",
    "    valid_indices = item_norms != 0\n",
    "    if not np.any(valid_indices):\n",
    "        logger.error(\"所有过滤后向量模长均为0\")\n",
    "        return []\n",
    "    # 计算相似度\n",
    "    dot_products = item_vectors[valid_indices].dot(query_vector)\n",
    "    similarities = dot_products / (item_norms[valid_indices] * query_norm)\n",
    "    # 6. 排序取Top K\n",
    "    index_sim_pairs = list(zip(np.where(valid_indices)[0], similarities))\n",
    "    index_sim_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_k_pairs = index_sim_pairs[:return_top_k]\n",
    "    # 7. 构造结果\n",
    "    results = []\n",
    "    for idx, sim in top_k_pairs:\n",
    "        item = filtered_data[idx]\n",
    "        results.append({\n",
    "            \"original_text\": item[\"original_text\"],\n",
    "            \"similarity\": round(float(sim), 4),\n",
    "            \"metadata\": item[\"metadata\"].copy(),\n",
    "            \"obj_key\": item.get(\"obj_key\", \"\")\n",
    "        })\n",
    "    logger.info(f\"检索完成，返回{len(results)}条结果\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d8cbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== 配置你的测试参数（无需修改，对应之前的测试文件） ======================\n",
    "TEST_FILE_TXT = \"test_blood_pressure.txt\"  # 你的txt测试文件（和Notebook同目录）\n",
    "TEST_FILE_JSON = \"test_nursing.json\"       # 你的json测试文件（和Notebook同目录）\n",
    "OBJ_KEY1 = \"health_record_bp_001.json\"     # 血压记录的Bucket对象名称\n",
    "OBJ_KEY2 = \"health_record_nursing_001.json\" # 护理记录的Bucket对象名称\n",
    "METADATA1 = {\"category\": \"blood_pressure\", \"date\": \"2023-10-27\", \"user_id\": \"test001\"}\n",
    "METADATA2 = {\"category\": \"nursing\", \"date\": \"2023-11-01\", \"user_id\": \"test001\"}\n",
    "# ===========================================================================================\n",
    "\n",
    "# 1. 测试：上传两个文件到普通Bucket\n",
    "logger.info(\"===== 开始上传文件到普通Bucket =====\")\n",
    "process_and_upload(TEST_FILE_TXT, OBJ_KEY1, METADATA1)\n",
    "process_and_upload(TEST_FILE_JSON, OBJ_KEY2, METADATA2)\n",
    "\n",
    "# 2. 测试1：检索血压记录（带元数据过滤）\n",
    "logger.info(\"\\n===== 测试1：带元数据过滤的血压记录检索 =====\")\n",
    "query1 = \"查找2023年10月的血压异常记录\"\n",
    "filter1 = {\"category\": \"blood_pressure\"}\n",
    "results1 = search_from_common_bucket(query1, filter1, return_top_k=1)\n",
    "if results1:\n",
    "    for res in results1:\n",
    "        print(f\"相似度：{res['similarity']}\")\n",
    "        print(f\"元数据：{res['metadata']}\")\n",
    "        print(f\"原始文本：{res['original_text']}\")\n",
    "else:\n",
    "    print(\"未找到血压相关记录\")\n",
    "\n",
    "# 3. 测试2：检索护理记录（带元数据过滤）\n",
    "logger.info(\"\\n===== 测试2：带元数据过滤的护理记录检索 =====\")\n",
    "query2 = \"查找2023年11月的护理记录\"\n",
    "filter2 = {\"category\": \"nursing\"}\n",
    "results2 = search_from_common_bucket(query2, filter2, return_top_k=1)\n",
    "if results2:\n",
    "    for res in results2:\n",
    "        print(f\"相似度：{res['similarity']}\")\n",
    "        print(f\"元数据：{res['metadata']}\")\n",
    "        print(f\"原始文本：{res['original_text']}\")\n",
    "else:\n",
    "    print(\"未找到护理相关记录\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
